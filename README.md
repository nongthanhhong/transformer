# **Transformer**
Implement Transformer with pytorch from scratch, using for Eng-Vi translation

Done:
- Base: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- Change order of Norm layer [On Layer Normalization in the Transformer Architecture](https://arxiv.org/abs/2002.04745)

- Using multi GPU to train

Future work:

- Try to use [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training](https://arxiv.org/abs/2305.14342)

- Train on TPU
